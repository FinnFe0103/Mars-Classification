{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 0. IMPORT STATEMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T11:36:39.088457Z",
     "iopub.status.busy": "2023-05-16T11:36:39.087984Z",
     "iopub.status.idle": "2023-05-16T11:36:54.029933Z",
     "shell.execute_reply": "2023-05-16T11:36:54.028817Z",
     "shell.execute_reply.started": "2023-05-16T11:36:39.088427Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install opencv-python\n",
    "#!pip install scikit-learn\n",
    "#!pip install imblearn\n",
    "import cv2\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 1. READ-IN DATA TO DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T11:36:54.031702Z",
     "iopub.status.busy": "2023-05-16T11:36:54.031244Z",
     "iopub.status.idle": "2023-05-16T11:36:54.289562Z",
     "shell.execute_reply": "2023-05-16T11:36:54.288871Z",
     "shell.execute_reply.started": "2023-05-16T11:36:54.031672Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10815, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ESP_013049_0950_RED-0067.jpg</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ESP_019697_2020_RED-0024.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ESP_015962_1695_RED-0016.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ESP_013049_0950_RED-0118.jpg</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ESP_015962_1695_RED-0017.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  label\n",
       "0   ESP_013049_0950_RED-0067.jpg      7\n",
       "7   ESP_019697_2020_RED-0024.jpg      1\n",
       "14  ESP_015962_1695_RED-0016.jpg      1\n",
       "21  ESP_013049_0950_RED-0118.jpg      7\n",
       "28  ESP_015962_1695_RED-0017.jpg      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"labels-map-proj_v3_2.txt\", sep=\" \", header=None, names=[\"image\", \"label\"])\n",
    "\n",
    "# filter all images that end with fv, brt, r90, r180, r270, and fh\n",
    "df = df[~df[\"image\"].str.contains(\"fv|brt|r90|r180|r270|fh\")]\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T11:36:54.290716Z",
     "iopub.status.busy": "2023-05-16T11:36:54.290476Z",
     "iopub.status.idle": "2023-05-16T11:36:54.298114Z",
     "shell.execute_reply": "2023-05-16T11:36:54.297290Z",
     "shell.execute_reply.started": "2023-05-16T11:36:54.290695Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    8802\n",
       "1     794\n",
       "6     298\n",
       "3     267\n",
       "4     250\n",
       "2     166\n",
       "7     164\n",
       "5      74\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 2. DELETE 8k RANDOM IMAGES & SHUFFLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T11:36:54.299235Z",
     "iopub.status.busy": "2023-05-16T11:36:54.299015Z",
     "iopub.status.idle": "2023-05-16T11:36:54.598870Z",
     "shell.execute_reply": "2023-05-16T11:36:54.598079Z",
     "shell.execute_reply.started": "2023-05-16T11:36:54.299214Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    1100\n",
       "1     794\n",
       "6     298\n",
       "3     267\n",
       "4     250\n",
       "2     166\n",
       "7     164\n",
       "5      74\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delte 7802 random images from the category with label 0\n",
    "df_us = df.drop(df[df[\"label\"] == 0].sample(7702, random_state=1).index)\n",
    "df_us[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T11:36:54.603872Z",
     "iopub.status.busy": "2023-05-16T11:36:54.603410Z",
     "iopub.status.idle": "2023-05-16T11:36:54.780689Z",
     "shell.execute_reply": "2023-05-16T11:36:54.779670Z",
     "shell.execute_reply.started": "2023-05-16T11:36:54.603844Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ESP_018720_2655_RED-0035.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ESP_046991_0950_RED-0024.jpg</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ESP_039350_1915_RED-0186.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ESP_014156_1865_RED-0023.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ESP_013049_0950_RED-0088.jpg</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          image  label\n",
       "0  ESP_018720_2655_RED-0035.jpg      2\n",
       "1  ESP_046991_0950_RED-0024.jpg      7\n",
       "2  ESP_039350_1915_RED-0186.jpg      1\n",
       "3  ESP_014156_1865_RED-0023.jpg      3\n",
       "4  ESP_013049_0950_RED-0088.jpg      7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle the dataframe\n",
    "df_us = df_us.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "df_us.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. READ-IN TO PIXELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T11:36:54.782264Z",
     "iopub.status.busy": "2023-05-16T11:36:54.781832Z",
     "iopub.status.idle": "2023-05-16T11:36:54.966809Z",
     "shell.execute_reply": "2023-05-16T11:36:54.965062Z",
     "shell.execute_reply.started": "2023-05-16T11:36:54.782237Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_images_labels_from_df(df, folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in range(len(df)):\n",
    "        img = cv2.imread(folder+\"/\"+df.iloc[i][0], cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "            labels.append(df.iloc[i][1])\n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T11:36:54.970305Z",
     "iopub.status.busy": "2023-05-16T11:36:54.969641Z",
     "iopub.status.idle": "2023-05-16T11:39:48.728440Z",
     "shell.execute_reply": "2023-05-16T11:39:48.726590Z",
     "shell.execute_reply.started": "2023-05-16T11:36:54.970258Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7] [1100  794  166  267  250   74  298  164]\n",
      "3113\n"
     ]
    }
   ],
   "source": [
    "X, y = load_images_labels_from_df(df_us, \"map-proj-v3_2\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(unique, counts)\n",
    "print(X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 4. NORMALIZE DATA TO 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T11:39:48.731267Z",
     "iopub.status.busy": "2023-05-16T11:39:48.730964Z",
     "iopub.status.idle": "2023-05-16T11:39:49.409356Z",
     "shell.execute_reply": "2023-05-16T11:39:49.408490Z",
     "shell.execute_reply.started": "2023-05-16T11:39:48.731240Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "X_norm = X / 255.0\n",
    "print(X_norm.min())\n",
    "print(X_norm.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 5. TRAIN-TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T11:39:49.410525Z",
     "iopub.status.busy": "2023-05-16T11:39:49.410284Z",
     "iopub.status.idle": "2023-05-16T11:39:49.947907Z",
     "shell.execute_reply": "2023-05-16T11:39:49.947066Z",
     "shell.execute_reply.started": "2023-05-16T11:39:49.410492Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1867, 227, 227) (1867,) Train\n",
      "(872, 227, 227) (872,) Test\n",
      "(374, 227, 227) (374,) Validation\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X_norm, y, test_size=0.4, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)\n",
    "print(X_train.shape, y_train.shape, \"Train\")\n",
    "print(X_test.shape, y_test.shape, \"Test\")\n",
    "print(X_val.shape, y_val.shape, \"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T11:39:49.949099Z",
     "iopub.status.busy": "2023-05-16T11:39:49.948891Z",
     "iopub.status.idle": "2023-05-16T11:39:49.957337Z",
     "shell.execute_reply": "2023-05-16T11:39:49.956642Z",
     "shell.execute_reply.started": "2023-05-16T11:39:49.949080Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([690, 452,  92, 168, 153,  41, 180,  91]))\n",
      "[0.36957686 0.24209963 0.04927691 0.08998393 0.08194965 0.02196036\n",
      " 0.09641136 0.0487413 ]\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([299, 238,  51,  66,  66,  24,  77,  51]))\n",
      "[0.34288991 0.27293578 0.05848624 0.07568807 0.07568807 0.02752294\n",
      " 0.08830275 0.05848624]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train, return_counts=True))\n",
    "print(np.unique(y_train, return_counts=True)[1]/y_train.shape[0])\n",
    "\n",
    "print(np.unique(y_test, return_counts=True))\n",
    "print(np.unique(y_test, return_counts=True)[1]/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 6. IMBALANCE HANDLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Flatten Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T11:39:49.958376Z",
     "iopub.status.busy": "2023-05-16T11:39:49.958186Z",
     "iopub.status.idle": "2023-05-16T11:39:50.282787Z",
     "shell.execute_reply": "2023-05-16T11:39:50.282025Z",
     "shell.execute_reply.started": "2023-05-16T11:39:49.958359Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1867, 227, 227)\n",
      "(1867, 51529)\n"
     ]
    }
   ],
   "source": [
    "X_reshaped = X_train.flatten().reshape(X_train.shape[0], 51529)\n",
    "print(X_train.shape)\n",
    "print(X_reshaped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T11:39:50.283787Z",
     "iopub.status.busy": "2023-05-16T11:39:50.283592Z",
     "iopub.status.idle": "2023-05-16T11:40:04.108328Z",
     "shell.execute_reply": "2023-05-16T11:40:04.107018Z",
     "shell.execute_reply.started": "2023-05-16T11:39:50.283769Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "adasyn = ADASYN(sampling_strategy=\"not majority\", random_state=1)\n",
    "X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_reshaped, y_train) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 OVERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T11:40:04.110434Z",
     "iopub.status.busy": "2023-05-16T11:40:04.110181Z",
     "iopub.status.idle": "2023-05-16T11:40:05.600431Z",
     "shell.execute_reply": "2023-05-16T11:40:05.599386Z",
     "shell.execute_reply.started": "2023-05-16T11:40:04.110410Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oversampler = RandomOverSampler(sampling_strategy=\"not majority\", random_state=3)\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X_reshaped, y_train) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T11:40:05.601720Z",
     "iopub.status.busy": "2023-05-16T11:40:05.601452Z",
     "iopub.status.idle": "2023-05-16T11:40:36.014098Z",
     "shell.execute_reply": "2023-05-16T11:40:36.013390Z",
     "shell.execute_reply.started": "2023-05-16T11:40:05.601698Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates for the first label after oversampling: 238\n",
      "Number of duplicates for all labels after oversampling: 3653\n"
     ]
    }
   ],
   "source": [
    "l1 = pd.DataFrame(X_resampled[np.where(y_resampled == 1)[0]])\n",
    "print(f\"Number of duplicates for the first label after oversampling: {l1.duplicated().sum()}\")\n",
    "all_l = pd.DataFrame(X_resampled)\n",
    "print(f\"Number of duplicates for all labels after oversampling: {all_l.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T11:40:36.016485Z",
     "iopub.status.busy": "2023-05-16T11:40:36.016246Z",
     "iopub.status.idle": "2023-05-16T11:40:36.024278Z",
     "shell.execute_reply": "2023-05-16T11:40:36.023448Z",
     "shell.execute_reply.started": "2023-05-16T11:40:36.016457Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5482, 51529)\n",
      "[0 1 2 3 4 5 6 7] [690 452  92 168 153  41 180  91]\n",
      "[0 1 2 3 4 5 6 7] [690 651 690 685 668 681 723 694]\n",
      "[0 1 2 3 4 5 6 7] [690 690 690 690 690 690 690 690]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "unique_a, counts_a = np.unique(y_train_adasyn, return_counts=True)\n",
    "unique_b, counts_b = np.unique(y_resampled, return_counts=True)\n",
    "print(X_train_adasyn.shape)\n",
    "print(unique, counts)\n",
    "print(unique_a, counts_a)\n",
    "print(unique_b, counts_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T11:40:36.025351Z",
     "iopub.status.busy": "2023-05-16T11:40:36.025157Z",
     "iopub.status.idle": "2023-05-16T11:40:36.271194Z",
     "shell.execute_reply": "2023-05-16T11:40:36.269438Z",
     "shell.execute_reply.started": "2023-05-16T11:40:36.025332Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training instances generated by ADASYN: \n",
      "(5482, 227, 227), correspoding y: (5482,)\n",
      "Shape of training instances generated by RandomOverSampler: \n",
      "(5520, 227, 227), correspoding y: (5520,)\n"
     ]
    }
   ],
   "source": [
    "X_t_A = X_train_adasyn.reshape(X_train_adasyn.shape[0], 227, 227) # type: ignore\n",
    "y_t_A = y_train_adasyn\n",
    "\n",
    "X_t_O = X_resampled.reshape(X_resampled.shape[0], 227, 227) # type: ignore\n",
    "y_t_O = y_resampled\n",
    "\n",
    "print(f\"Shape of training instances generated by ADASYN: \\n{X_t_A.shape}, correspoding y: {y_t_A.shape}\")\n",
    "print(f\"Shape of training instances generated by RandomOverSampler: \\n{X_t_O.shape}, correspoding y: {y_t_O.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 7. AUGMENTATION\n",
    "1. Rotate 90\n",
    "2. Rotate 180\n",
    "3. Rotate 270\n",
    "4. Flip Horizontally\n",
    "5. Flip Vertically\n",
    "6. Zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T11:40:36.274792Z",
     "iopub.status.busy": "2023-05-16T11:40:36.273867Z",
     "iopub.status.idle": "2023-05-16T11:40:36.406386Z",
     "shell.execute_reply": "2023-05-16T11:40:36.404951Z",
     "shell.execute_reply.started": "2023-05-16T11:40:36.274735Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zoom_at(img, zoom=1.0):\n",
    "    h, w, = [zoom * i for i in img.shape]\n",
    "    cx, cy = w/2, h/2\n",
    "    img = cv2.resize( img, (0, 0), fx=zoom, fy=zoom)\n",
    "    img = img[int(round(cy - h/zoom * .5)) : int(round(cy + h/zoom * .5)),\n",
    "              int(round(cx - w/zoom * .5)) : int(round(cx + w/zoom * .5))]\n",
    "    return img\n",
    "\n",
    "def augment_images(images, labels):\n",
    "    \"\"\"\n",
    "    Augments a numpy array of images by rotating them by 90, 180, and 270 degrees, flipping them horizontally and\n",
    "    vertically, and zooming in on them.\n",
    "    \n",
    "    Args:\n",
    "    - images (numpy.ndarray): A numpy array of shape (N, H, W, C), where N is the number of images, H and W are the\n",
    "                              height and width of the images, and C is the number of color channels.\n",
    "                              \n",
    "    Returns:\n",
    "    - augmented_images (numpy.ndarray): A numpy array of shape (7N, H, W, C), where the first N images are the\n",
    "                                         original images and the remaining 6N images are the augmented images.\n",
    "    \"\"\"\n",
    "    # Initialize an empty numpy array to store the augmented images\n",
    "    augmented_images = np.empty((7*images.shape[0], images.shape[1], images.shape[2]))\n",
    "    augmented_labels = np.empty((7*images.shape[0]))\n",
    "    \n",
    "\n",
    "    # Loop through each image in the array and perform the augmentations\n",
    "    for i in range(images.shape[0]):\n",
    "        image = images[i]\n",
    "        label = labels[i]\n",
    "\n",
    "        # Add the original image to the output array\n",
    "        augmented_images[(i*7)] = image\n",
    "        augmented_labels[(i*7)] = label\n",
    "        \n",
    "        # Rotate the image by 90 degrees\n",
    "        rotated_90 = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
    "        augmented_images[(i*7)+1] = rotated_90\n",
    "        augmented_labels[(i*7)+1] = label\n",
    "        \n",
    "        # Rotate the image by 180 degrees\n",
    "        rotated_180 = cv2.rotate(image, cv2.ROTATE_180)\n",
    "        augmented_images[(i*7)+2] = rotated_180\n",
    "        augmented_labels[(i*7)+2] = label\n",
    "        \n",
    "        # Rotate the image by 270 degrees\n",
    "        rotated_270 = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        augmented_images[(i*7)+3] = rotated_270\n",
    "        augmented_labels[(i*7)+3] = label\n",
    "        \n",
    "        # Flip the image horizontally\n",
    "        flipped_horizontal = cv2.flip(image, 1)\n",
    "        augmented_images[(i*7)+4] = flipped_horizontal\n",
    "        augmented_labels[(i*7)+4] = label\n",
    "        \n",
    "        # Flip the image vertically\n",
    "        flipped_vertical = cv2.flip(image, 0)\n",
    "        augmented_images[(i*7)+5] = flipped_vertical\n",
    "        augmented_labels[(i*7)+5] = label\n",
    "        \n",
    "        # Zoom in on the image\n",
    "        zoomed_in = zoom_at(image, 1.2)\n",
    "        augmented_images[(i*7)+6] = zoomed_in\n",
    "        augmented_labels[(i*7)+6] = label\n",
    "\n",
    "        #print(f\"Image, {i} done\")\n",
    "\n",
    "    return augmented_images, augmented_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T11:40:36.410218Z",
     "iopub.status.busy": "2023-05-16T11:40:36.409103Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_t_A_A, y_t_A_A = augment_images(X_t_A, y_t_A)\n",
    "X_t_O_A, y_t_O_A = augment_images(X_t_O, y_t_O)\n",
    "print(f\"Shape of original training data for ADASYN: {X_t_A.shape} with corresponding y: {y_t_A.shape}\")\n",
    "print(f\"Shape of augmented training data for ADASYN: {X_t_A_A.shape} with corresponding y: {y_t_A_A.shape}\")\n",
    "print(f\"Shape of original training data for RandomOverSampler: {X_t_O.shape} with corresponding y: {y_t_O.shape}\")\n",
    "print(f\"Shape of augmented training data for RandomOverSampler: {X_t_O_A.shape} with corresponding y: {y_t_O_A.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aug_names = [\"Original\", \"Rotate90\", \"Rotate180\", \"Rotate270\", \"Flip Horizontal\", \"Flip Vertical\", \"Zoom\"]\n",
    "class_names = [\"other\", \"crater\", \"dark dune\", \"slope streak\", \"bright dune\", \"impact ejecta\", \"swiss cheese\", \"spider\"]\n",
    "\n",
    "# function that generates 10 random integers from numpy array that are divisible by 7\n",
    "def generate_random_integers(X, n):\n",
    "    random_integers = []\n",
    "    while len(random_integers) < n:\n",
    "        integer = np.random.randint(0, len(X))\n",
    "        if integer % 7 == 0:\n",
    "            random_integers.append(integer)\n",
    "    return random_integers\n",
    "\n",
    "# create function that takes 10 random images from the augmented training data and plots them\n",
    "def plot_augmented_images(X, y, aug_names, class_names, num_images):\n",
    "    indices = generate_random_integers(X, num_images)\n",
    "    fig, axes = plt.subplots(len(indices), 7, figsize=(15, 15))\n",
    "    c = 0\n",
    "    for i in indices:\n",
    "        for j in range(7):\n",
    "            axes[c, j].imshow(X[i+j], cmap=\"gray\")\n",
    "            axes[c, j].set_xticks([])\n",
    "            axes[c, j].set_yticks([])\n",
    "            \n",
    "            axes[c, j].set_xlabel(aug_names[j] + \" \" + str(int(i+j)))\n",
    "            if j == 0:\n",
    "                axes[c, j].set_ylabel(class_names[int(y[i+j])])\n",
    "        c += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_augmented_images(X_t_A_A, y_t_A_A, aug_names, class_names, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 8 Apply Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "import datetime\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, Input, AveragePooling2D, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "np.random.seed(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def shuffle_data(x, y):\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    p = np.random.permutation(x.shape[0])\n",
    "    return x[p], y[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_evaluation(model, history, X_test, y_test, class_names):\n",
    "    sns.lineplot(data=history.history['accuracy'], label='accuracy')\n",
    "    sns.lineplot(data=history.history['val_accuracy'], label='val_accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0.5, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
    "    print(\"Accuracy on the training set:\", test_acc)\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred_class = y_pred_prob.argmax(axis=-1)\n",
    "\n",
    "    print(classification_report(y_pred_class, y_test, target_names=class_names))\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(confusion_matrix(y_pred_class, y_test), xticklabels=class_names, yticklabels=class_names, annot=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ALEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_alex(X_train, y_train, X_val, y_val, X_test, y_test, learning_rate, weight_decay, momentum, batch_size, epochs, class_names, folder):\n",
    "    alex = Sequential()\n",
    "    \n",
    "    alex.add(Conv2D(filters=96, input_shape=(227, 227, 1), kernel_size=(11,11), strides=(4,4), padding='valid', activation='relu'))\n",
    "    alex.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "    alex.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='valid', activation='relu'))\n",
    "    alex.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "    alex.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu'))\n",
    "    alex.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu'))\n",
    "    alex.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu'))\n",
    "    alex.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "    alex.add(Flatten())\n",
    "    alex.add(Dense(4096, input_shape=(227*227*1,), activation='relu'))\n",
    "    alex.add(Dropout(0.5))\n",
    "    alex.add(Dense(4096, activation='relu'))\n",
    "    alex.add(Dropout(0.5))\n",
    "    alex.add(Dense(8))\n",
    "    alex.add(Activation('softmax'))\n",
    "    \n",
    "    alex.summary()\n",
    "    \n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate, decay=weight_decay, momentum=momentum, nesterov=True)\n",
    "    loss_function = keras.losses.SparseCategoricalCrossentropy()\n",
    "    \n",
    "    alex.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
    "    \n",
    "    # Tensorboard\n",
    "    log_dir = folder + \"/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    history = alex.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val))\n",
    "\n",
    "    model_evaluation(alex, history, X_test, y_test, class_names)\n",
    "\n",
    "    return alex, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GOOGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inception_module(x, filters_1x1, filters_3x3_reduce, filters_3x3, filters_5x5_reduce, filters_5x5, filters_pool_proj):\n",
    "    path1 = Conv2D(filters_1x1, (1, 1), padding='same', activation='relu')(x)\n",
    "    path2 = Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu')(x)\n",
    "    path2 = Conv2D(filters_3x3, (3, 3), padding='same', activation='relu')(path2)\n",
    "    path3 = Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu')(x)\n",
    "    path3 = Conv2D(filters_5x5, (5, 5), padding='same', activation='relu')(path3)\n",
    "    path4 = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    path4 = Conv2D(filters_pool_proj, (1, 1), padding='same', activation='relu')(path4)\n",
    "    return concatenate([path1, path2, path3, path4], axis=-1)\n",
    "\n",
    "def GoogLeNet():\n",
    "    input_layer = Input(shape=(227, 227, 1))\n",
    "\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2), padding='same', activation='relu')(input_layer)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = Conv2D(64, (1, 1), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(192, (3, 3), strides=(1, 1), padding='same', activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = inception_module(x, 64, 96, 128, 16, 32, 32)\n",
    "    x = inception_module(x, 128, 128, 192, 32, 96, 64)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = inception_module(x, 192, 96, 208, 16, 48, 64)\n",
    "    x = inception_module(x, 160, 112, 224, 24, 64, 64)\n",
    "    x = inception_module(x, 128, 128, 256, 24, 64, 64)\n",
    "    x = inception_module(x, 112, 144, 288, 32, 64, 64)\n",
    "    x = inception_module(x, 256, 160, 320, 32, 128, 128)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = inception_module(x, 256, 160, 320, 32, 128, 128)\n",
    "    x = inception_module(x, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "    x = AveragePooling2D(pool_size=(7, 7), strides=(1, 1), padding='valid')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1000, activation='sigmoid')(x)\n",
    "    output_layer = Dense(8, activation='softmax')(x)\n",
    "\n",
    "    # Define the model\n",
    "    model_googlenet = keras.Model(input_layer, output_layer)\n",
    "\n",
    "    # Return the model\n",
    "    return model_googlenet\n",
    "\n",
    "def run_google(X_train, y_train, X_val, y_val, X_test, y_test, learning_rate, momentum, batch_size, epochs, class_names, folder):\n",
    "    google = GoogLeNet()\n",
    "    \n",
    "    optimizer = keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum, nesterov=True)\n",
    "    loss_function = keras.losses.SparseCategoricalCrossentropy()\n",
    "    \n",
    "    google.compile(optimizer=optimizer, loss=loss_function, metrics=[\"accuracy\"])\n",
    "    \n",
    "    # Tensorboard\n",
    "    log_dir = folder + \"/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    history = google.fit(X_t_A_A_S, y_t_A_A_S, batch_size=32, epochs=10, validation_data=(X_val, y_val))\n",
    "    \n",
    "    model_evaluation(google, history, X_test, y_test, class_names)\n",
    "    \n",
    "    return google, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 AlexNet Oversampled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-16T10:58:06.148313Z",
     "iopub.status.busy": "2023-05-16T10:58:06.147538Z",
     "iopub.status.idle": "2023-05-16T11:07:20.664194Z",
     "shell.execute_reply": "2023-05-16T11:07:20.660725Z",
     "shell.execute_reply.started": "2023-05-16T10:58:06.148276Z"
    },
    "tags": []
   },
   "source": [
    "class_names = [\"other\", \"crater\", \"dark dune\", \"slope streak\", \"bright dune\", \"impact ejecta\", \"swiss cheese\", \"spider\"]\n",
    "\n",
    "X_t_A_A_S, y_t_A_A_S, = shuffle_data(X_t_A_A, y_t_A_A)\n",
    "X_t_O_A_S, y_t_O_A_S, = shuffle_data(X_t_O_A, y_t_O_A)\n",
    "\n",
    "folder = \"logs_Alex_Over\"\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs_Alex_Over/\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.0005\n",
    "momentum = 0.9\n",
    "batch_size = 32 #128\n",
    "epochs = 1 #90\n",
    "\n",
    "alex_O, history_alex_O = run_alex(X_train=X_t_O_A_S, y_train=y_t_O_A_S, \n",
    "                                 X_val=X_val, y_val=y_val,\n",
    "                                 X_test=X_test, y_test=y_test,\n",
    "                                 learning_rate=learning_rate, weight_decay=weight_decay, momentum=momentum,\n",
    "                                 batch_size=batch_size, epochs=epochs,\n",
    "                                 class_names=class_names, folder=folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2 AlexNet ADASYN Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class_names = [\"other\", \"crater\", \"dark dune\", \"slope streak\", \"bright dune\", \"impact ejecta\", \"swiss cheese\", \"spider\"]\n",
    "\n",
    "X_t_O_A_S, y_t_O_A_S, = shuffle_data(X_t_O_A, y_t_O_A)\n",
    "X_t_A_A_S, y_t_A_A_S, = shuffle_data(X_t_A_A, y_t_A_A)\n",
    "\n",
    "folder = \"logs_Alex_ADASYN\"\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs_Alex_ADASYN/\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.0005\n",
    "momentum = 0.9\n",
    "batch_size = 32 #128\n",
    "epochs = 1 #90\n",
    "\n",
    "alex_O, history_alex_O = run_alex(X_train=X_t_A_A_S, y_train=y_t_A_A_S, \n",
    "                                 X_val=X_val, y_val=y_val,\n",
    "                                 X_test=X_test, y_test=y_test,\n",
    "                                 learning_rate=learning_rate, weight_decay=weight_decay, momentum=momentum,\n",
    "                                 batch_size=batch_size, epochs=epochs,\n",
    "                                 class_names=class_names, folder=folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3 GoogleNet Oversampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_names = [\"other\", \"crater\", \"dark dune\", \"slope streak\", \"bright dune\", \"impact ejecta\", \"swiss cheese\", \"spider\"]\n",
    "\n",
    "X_t_O_A_S, y_t_O_A_S, = shuffle_data(X_t_O_A, y_t_O_A)\n",
    "X_t_A_A_S, y_t_A_A_S, = shuffle_data(X_t_A_A, y_t_A_A)\n",
    "\n",
    "folder = \"logs_Google_Over\"\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs_Google_Over/\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "batch_size = 32 #128\n",
    "epochs = 1 #90\n",
    "\n",
    "google_O, history_google_O = run_google(X_train=X_t_O_A_S, y_train=y_t_O_A_S, \n",
    "                                        X_val=X_val, y_val=y_val,\n",
    "                                        X_test=X_test, y_test=y_test,\n",
    "                                        learning_rate=learning_rate, momentum=momentum,\n",
    "                                        batch_size=batch_size, epochs=epochs,\n",
    "                                        class_names=class_names, folder=folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4 GoogleNet ADASYN Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class_names = [\"other\", \"crater\", \"dark dune\", \"slope streak\", \"bright dune\", \"impact ejecta\", \"swiss cheese\", \"spider\"]\n",
    "\n",
    "X_t_O_A_S, y_t_O_A_S, = shuffle_data(X_t_O_A, y_t_O_A)\n",
    "X_t_A_A_S, y_t_A_A_S, = shuffle_data(X_t_A_A, y_t_A_A)\n",
    "\n",
    "folder = \"logs_Google_ADASYN\"\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs_Google_ADASYN/\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "batch_size = 32 #128\n",
    "epochs = 1 #90\n",
    "\n",
    "google_O, history_google_O = run_google(X_train=X_t_A_A_S, y_train=y_t_A_A_S, \n",
    "                                        X_val=X_val, y_val=y_val,\n",
    "                                        X_test=X_test, y_test=y_test,\n",
    "                                        learning_rate=learning_rate, momentum=momentum,\n",
    "                                        batch_size=batch_size, epochs=epochs,\n",
    "                                        class_names=class_names, folder=folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
